{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  K-means algorithm is a typical clustering algorithm based on distance, using distance as the evaluation measurement of similarity.   Due to the simple theory, easy interpretability and implement, K-means ranks top ten machine learning algorithms. However, some drawbacks also influence the popularity of it. For example, the number of cluster k needs to be specified in advance, K-means algorithm is sensitive to initialization, etc. \n",
    "  \n",
    "  To address the problem of initialization, K-means++ performs well. This modified algorithm chooses K initial clustering centers according to the following ideas: assume n centers (0 < n < K) have been fixed, the center n + 1 would be select with the probability which is higher when the point is farther away from previous n centers. Although K-means++ provides a method to set initial centers, its inherent sequential nature is still troublesome. \n",
    "  \n",
    "  This paper provides a solution, K-means|| algorithm, implement it as well as K-means++ in both sequential and parallel settings and compare both algorithms in several aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background\n",
    "\n",
    "## 2.1 Bayesian Model\n",
    "prior + likelihood = posterior\n",
    "\n",
    "\n",
    "## 2.2 Dirichlet distribution and Multinomial distribution\n",
    "\n",
    "Conjugate distribution intro....\n",
    "$$\n",
    "multi(m_1,m_2,m_3\\mid n, p_1,p_2,p_3) = \\frac{n!}{m_1!m_2!m_3!}p_1^{m_1}p_2^{m_2}p_3^{m_3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Dirichlet(p_1,p_2,p_3\\mid \\alpha_1,\\alpha_2,\\alpha_3) = \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}p_1^{\\alpha_1-1}p_2^{\\alpha_2-1}p_3^{\\alpha_3-1}\n",
    "$$\n",
    "\n",
    "For higher dimension, we have\n",
    "\n",
    "$$\n",
    "Dirichlet(\\vec{p}\\mid \\vec{\\alpha}) = \\frac{\\Gamma(\\sum_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\\prod_{k=1}^K\n",
    "p_k^{\\alpha_k-1}$$\n",
    "\n",
    "With the same conjugate relationship, we have:\n",
    "\n",
    "$$\n",
    "Dirichlet(\\vec{p}\\mid\\vec{\\alpha})+Multi(\\vec{m}) = Dirichlet(\\vec{p}\\mid\\vec{\\alpha}+\\vec{m})\n",
    "$$\n",
    "\n",
    "And the expectation of Dirichlet distribution is:\n",
    "\n",
    "$$\n",
    "E(Diri(\\vec{p}\\mid\\vec{\\alpha}) = \\left( \\frac{\\alpha_1}{\\sum_{k=1}^K\\alpha_k},\\frac{\\alpha_2}{\\sum_{k=1}^K\\alpha_k},...\\frac{\\alpha_K}{\\sum_{k=1}^K\\alpha_k}  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 LDA Model\n",
    "\n",
    "  We use the language of text collections throughout the paper, referring to entities such as “words,” “documents,” and “corpora.”(p995) Our goal is to find the topic dictribution of each document and the word distribution of each topic. Also, if we are given a new document, we need to use some parameters trained from our model, to inference the topic of taht new document.\n",
    "  \n",
    "  To mimic the process of human wrinting an article, the author always needs to have some fixed topics and then uses words withch are strongly related with those topics. So when it comes to machine learning, we also pretend that we need to have the topic at first and then we can generate words under that topic.\n",
    "\n",
    "To describe the algorithm in an aesier way to understanding, we can treat distribution as a multi-sided dice, each side represents a topic(word), the probabilities of being up of each side are defferent. Added Bayesian idea, the LDA algorihm can be showed as:\n",
    "- There are two big boxes, one is full of \"doc-topic\" dices, and another box is full of \"topic-word\" dices.\n",
    "- Pick $K$ \"topic-word\" dices from the second box, label tham as $1,2,...,K$.\n",
    "- For each document, we pick one \"doc-topic\" dice from the first box and repeat followings to get words:\n",
    "   - through this dice, obtain the topic $z$\n",
    "   - chooes the \"doc-topic\" dice whose lable is $z$ and through it, then we can obtain one word\n",
    "   \n",
    "   \n",
    "In other words, LDA algorithm assumes the following generative process for each document $\\overrightarrow{w}$:\n",
    "\n",
    "1. Choose $N \\sim Poisson(\\xi)$\n",
    "2. Choose $\\theta \\sim Dir(\\alpha)$\n",
    "3. For each of the $N$ words $w_n$:\n",
    "\n",
    "    (a). Choose a topic $z_n \\sim Multi(\\theta)$\n",
    "    \n",
    "    (b). Choose a word $w_n$ from $p(w_n\\mid z_n, \\beta)$, a multinomal probability conditioned on th etopic $z_n$.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Mathematical analysis\n",
    "\n",
    "First of all, let's define some terms which will use in the following text:\n",
    "\n",
    "$\\cdot$ $D$(a corpus) is the collection of $M$ documents;\n",
    "\n",
    "$\\cdot$ $V$ is the total number of words from a vocabulary;\n",
    "\n",
    "$\\cdot$ $M$ is the number of documents;\n",
    "\n",
    "$\\cdot$ $K$ is number of topics of the corpus;\n",
    "\n",
    "$\\cdot$ $N_m$ is the number of words in the $m^{th} document$;\n",
    "\n",
    "$\\cdot$ $w_{mn}$ is the  $n^{th}$ word in docuent $m$; \n",
    "\n",
    "$\\cdot$ $z_{mn}$ is the topic of word $w_{mn}$;\n",
    "\n",
    "$\\cdot$ $\\theta_m$ is the distribution for document $m$;\n",
    "\n",
    "$\\cdot$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that the prior distribution of topics is a Dirichlet distribution, i.e., for any document $d$, the distribution of its topic is\n",
    "\n",
    "$$\n",
    "\\theta_d = Dirichlet(\\vec{\\alpha})\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a K-dimension vector.\n",
    "\n",
    "A k-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector  lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i=1$), and has the following probability density on the simplex:\n",
    "$$\n",
    "p(\\theta\\mid \\alpha) = \\frac{\\Gamma(\\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\theta_1^{\\alpha_1-1}...\\theta_k^{\\alpha_k-1}\n",
    "$$\n",
    "\n",
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\textbf{z} = (z_1, z_2,...z_N)$ and a set of $N$ words $\\textbf{w} = (w_1, w_2,...w_N)$ is given by:\n",
    "$$\n",
    "p(\\theta, \\textbf{z},\\textbf{w}\\mid \\alpha, \\beta) = p(\\theta\\mid\\alpha)\\prod_{n=1}^Np(z_n\\mid \\theta)p(w_n\\mid z_n,\\beta)\n",
    "$$\n",
    "\n",
    "Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document $\\textbf{w}$:\n",
    "$$\n",
    "p(\\textbf{w}\\mid \\alpha, \\beta) = \\int p(\\theta\\mid\\alpha)\\prod_{n=1}^N \\sum_{z_n} p(z_n\\mid \\theta)p(w_n\\mid z_n,\\beta)d\\,\\theta\n",
    "$$\n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus $\\textbf{D} = (\\textbf{w}_1, \\textbf{w}_2,...\\textbf{w}_M)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\textbf{D}\\mid \\alpha,\\beta) &= \\prod_{d=1}^M p(\\textbf{w}_d\\mid \\alpha,\\beta)\\\\\n",
    "&= \\prod_{d=1}^M \\int p(\\theta_d\\mid\\alpha) \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}\\mid \\theta)p(w_{dn}\\mid z_{dn},\\beta)d\\,\\theta_d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Variational inference (E-step)\n",
    "\n",
    "### Parameter declaration\n",
    "\n",
    "v: vocabulary, $v = 1,2,...,V$\n",
    "\n",
    "z: topic , $z = 1,2,...,K$\n",
    "\n",
    "m: document, $m = 1,2,...,M$\n",
    "\n",
    "w: word, $w =1,2,...N_1, N_1+1, ...N_1+N_2,...,\\sum_{m=1}^NN_m$ \n",
    "\n",
    "$\\alpha$: a $K$ dim vector, represents the topic distribution of $K$ topics\n",
    "\n",
    "$\\beta$: a $K\\times V$ matrix, $\\beta_{ij}$ represents probability of the $j^{th}$ word of $i^{th}$ topic.\n",
    "\n",
    "$\\phi$: a $M\\times N \\times K$ matrix, where $N = \\sum_{m=1}^NN_m$, represents probability of each topic for words in each document\n",
    "\n",
    "$\\gamma$: a $M\\times K$ matrix, represents the probability of topic for each document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We need to compute the posterior distribution of hidden variables:\n",
    "\n",
    "$$\n",
    "p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta) = \\frac{p(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)}{p(\\textbf{w}\\mid \\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "But this distribution is intractable due to the coupling between $\\theta$ and $\\beta$.\n",
    "So we use the variation distribution on latent variables:\n",
    "\n",
    "$$\n",
    "q(\\theta, \\textbf{z}\\mid \\gamma, \\phi) = q(\\theta\\mid \\gamma)\\prod_{n=1}^N q(z_n\\mid \\phi_n)\n",
    "$$\n",
    "\n",
    "An optimization problem that determins the values of $\\gamma$ and $\\phi$ with respect to KL-Divergence:\n",
    "\n",
    "$$\n",
    "(\\gamma^*, \\phi^*) = arg\\ \\underset{\\phi, \\gamma}{min}\\,D(q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta))\n",
    "$$\n",
    "\n",
    "Then we denote $q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)$ by $q$, the KL-Divergence between $q$ and $p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D(q||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)) &= E_q[log\\,q]-E_q[log\\,p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)]\\\\\n",
    "&=E_q[log\\,q]-E_q[log\\,p(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)]+log\\,p(\\textbf{w}\\mid \\alpha, \\beta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using Jensen's inequality, we have the bound as $log\\,p(\\textbf{w}\\mid \\alpha, \\beta)\\geq E_q[log\\,p(\\theta, \\textbf{z},\\textbf{w}\\mid \\alpha, \\beta)]-E_q[log(\\theta, \\textbf{z})]$. And by denoting $E_q[log(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)] - E_q[log\\,q(\\theta, \\textbf{z})]$ by $L(\\gamma, \\phi; \\alpha, \\beta)$, we have\n",
    "\n",
    "$$\n",
    "log\\,P(\\textbf{w}\\mid \\alpha, \\beta) = L(\\gamma, \\phi; \\alpha, \\beta)+D(q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta))\n",
    "$$\n",
    "\n",
    "So minimizeing the KL-Divergence $D$ is equivalent to maximizing likelihood $L$, then we need to maximize the following five parts one by one:\n",
    "\n",
    "$$\n",
    "L(\\gamma, \\phi;\\alpha, \\beta) = E_q[log\\,p(\\theta\\mid \\alpha)]+E_q[log\\,p(\\textbf{z}\\mid\\theta)]+E_q[log\\,p(\\textbf{w}\\mid\\textbf{z},\\beta)]-E_q[log\\,q(\\theta)-E_q[log\\,q(\\textbf{z})]]\n",
    "$$\n",
    "\n",
    "Finally, by computing the derivatives of $L$ and set them equals to 0, we obtain update equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi_{ni}&\\propto \\beta_{iv}\\,exp\\{\\psi(\\gamma_i)-\\psi(\\sum_{j=1}^K\\gamma_j)\\}\\\\\n",
    "\\gamma_i &= \\alpha_1+\\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\psi$ is the digamma function, the first derivative of the log Gamma function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference Algorithm\n",
    "\n",
    "- initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all $i$ and $n$.\n",
    "- initialize $\\gamma_i^0 = \\alpha_i+\\frac{N}{K}$ for all $i$.\n",
    "- repeat\n",
    "    - for $n = 1$ to $N$\n",
    "    - for $i = 1$ to $K$\n",
    "         - $\\phi_{ni}^{t+1} = \\beta_{iw_n}\\,exp\\{\\psi(\\gamma_i^t)\\}$.\n",
    "         - normalize $\\phi_{ni}^{t+1}$ to sum 1.\n",
    "    - $\\gamma^{t+1} = \\alpha+\\sum_{n=1}^N \\phi_n^{t+1}$.\n",
    "- until convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence function\n",
    "def is_convergence(old, new, tol = 10**(-5)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(np.sum(np.square(old, new)))\n",
    "    return loss <= tol\n",
    "\n",
    "def optimize_vp(phi, gamma, alpha, beta, M, N, K):\n",
    "    '''\n",
    "    return\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : int or sequence of ints\n",
    "        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
    "    gamma : data-type, optional\n",
    "        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
    "        `numpy.float64`.\n",
    "    words : list \n",
    "        the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #update phi\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(K):\n",
    "                phi[m][n,i] = beta[words[m][n],i] * np.exp(digamma(gamma[m,i]))\n",
    "            #nomalize to 1\n",
    "            phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "            \n",
    "    #update gamma\n",
    "    gamma = np.zeros((M, K))\n",
    "    for i in range(k):\n",
    "        gamma[i]  = alpha[i] + np.sum(phi[i], axis = 0)\n",
    "   \n",
    "    return phi, gamma\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.3 Parameter estimation (M-step)\n",
    "\n",
    "Now, given a corpus of documents $\\textbf{D} = (\\textbf{w}_1, \\textbf{w}_2,...\\textbf{w}_M)$, we hope to find proper parameters. So, in this M-step, maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$.\n",
    "\n",
    "Firstly, we maximize $L(\\gamma, \\phi;\\alpha,\\beta)$ with respect to $\\beta$, by taking the derivative with respect to $\\beta_{ij}$ and setting it to zero, we have:\n",
    "\n",
    "$$\n",
    "\\beta_{ij}\\propto \\sum_{m=1}^M\\sum_{n=1}^{N_m}\\phi_{m,n,i}w_{mn}^j\n",
    "$$\n",
    "\n",
    "here we need to attention that $\\beta$ is a probability matrix whose rows represent words probability of one particular topic, so the sum of each rows should equal one.\n",
    "\n",
    "Secondly, we maximize $L(\\gamma, \\phi;\\alpha,\\beta)$ with respect to $\\alpha$, and then take the derivative with respect to $\\alpha_i$. However, it is difficult to compute $\\alpha_i$ by setting the derivative to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} = M\\left(\\psi(\\sum_{j=1}^K \\alpha_i)-\\psi(\\alpha_i)\\right)+\\sum_{m=1}^M \\left(\\psi_{mi}-\\psi\\left(\\sum_{j=1}^K \\gamma_{mj}\\right)\\right):=g(\\alpha_i)\n",
    "$$\n",
    "\n",
    "Because this derivative depends on $\\alpha_i$, where $j\\neq i$. So we need to use an iterative method (eg. Newton-Raphson method) to find the extreme point. Then we can compute the Hessian Matrix as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2L}{\\partial \\alpha_i\\partial\\alpha_j} = M\\left(\\psi'\\left(\\sum_{j=1}^K \\alpha_i\\right)-\\delta(i,j)\\psi'(\\alpha_i)\\right):=H(\\alpha_i)\n",
    "$$\n",
    "\n",
    "Finally, we only need to use Newton-Raphson methods to update $\\alpha$ and obtain the answer when converge. The Newton-Raphson optimization technique finds a stationary point of a function by iterating\n",
    "\n",
    "$$\n",
    "\\alpha_{new} = \\alpha_{old}-H(\\alpha_{old})^{-1}g(\\alpha_{old})\n",
    "$$\n",
    "\n",
    "where $H(\\alpha)$ is the Hessian matrix and $g(\\alpha)$ is the gradient vector.\n",
    "If the Hessian matrix is of form $H = diag(h)+\\textbf{1}\\,z\\,\\textbf{1}^T$, then we can culculate that\n",
    "\n",
    "$$\n",
    "(H^{-1}g)_i = \\frac{g_i-c}{h_i}\\\\\n",
    "c = \\frac{\\sum_{j=1}^K g_j/h_j}{z^{-1}+\\sum_{j=1}^K h_j^{-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alpha\n",
    "def alphaa_estimate(gamma, alpha_initial, K, M):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    from scipy.special import digamma, polygamma\n",
    "    \n",
    "    # set the maximum iteration\n",
    "    iteration = 1000\n",
    "    for t in range(iteration):\n",
    "        alpha_old = alpha\n",
    "        # compute the gradient vector\n",
    "        g = []\n",
    "        for i in range(len(alpha)):\n",
    "            p1 = M*(digamma(np.sum(alpha))-digamma(alpha[i]))\n",
    "            for d in range(M):\n",
    "                p2 =+ digamma(gamma[d,i])-digamme(np.sum(gamma[d,:]))\n",
    "            new = p1+p2\n",
    "            g = g.append(new)\n",
    "        \n",
    "        # H = diag(h)+1z1.T\n",
    "        # compute the diagonal part of the Hessian matrix\n",
    "        h = -M*polygamma(1,alpha)\n",
    "        # compute the constant part of the Hessian matrix\n",
    "        z = M*polygamma(1,np.sum(alpha))\n",
    "        # compute the constant c\n",
    "        c = np.sum(g/h)/(z**(-1)+np.sum(h**(-1)))\n",
    "        \n",
    "        # update alpha\n",
    "        alpha =- (g-c)/h\n",
    "        \n",
    "        if is_covergence(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate beta\n",
    "def beta_estimate(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    V = len(V_words)\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0).T\n",
    "    \n",
    "    # then normalize each column s.t. the column sum is one\n",
    "    for i in range(K):\n",
    "        beta[i,:] = beta[i,:]/sum(beta[i,:])\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Combination (Variational EM)\n",
    "\n",
    "After the procedures above, now we can combine all the algorithms into a summary:\n",
    "### (1) input:\n",
    "- K: the number of topics\n",
    "- M: the number of documents\n",
    "- D: contains all words of documents\n",
    "\n",
    "### (2) initialize $\\alpha$ and $\\beta$\n",
    "\n",
    "### (3) start EM algorithm:\n",
    "- Use variational inference algorithm to update $\\gamma$ and $\\phi$:\n",
    "- (1). initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all $i$ and $n$.\n",
    "- (2). initialize $\\gamma_i^0 = \\alpha_i+\\frac{N}{K}$ for all $i$.\n",
    "\n",
    "   - repeat\n",
    "   - for $n = 1$ to $N$\n",
    "   - for $i = 1$ to $K$\n",
    "       - $\\phi_{ni}^{t+1} = \\beta_{iw_n}\\,exp\\{\\psi(\\gamma_i^t)\\}$.\n",
    "       - normalize $\\phi_{ni}^{t+1}$ to sum 1.\n",
    "   - $\\gamma^{t+1} = \\alpha+\\sum_{n=1}^N \\phi_n^{t+1}$.\n",
    "   - until convergence\n",
    "     \n",
    "- Use parameter estimation to estimate $\\alpha$ and $\\beta$:\n",
    "   - use the converged $\\phi$ and $\\gamma$ to update\n",
    "   - until convergence\n",
    "   \n",
    "### (4) when all parameters ocnvergede, break.\n",
    "\n",
    "### (5) output posterior parameters $\\alpha$ and $\\beta$, end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Experience test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
