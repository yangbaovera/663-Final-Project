{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Latent Dirichlet allocation(LDA) is a generative topic probabilistic model first proposed by  David M.Blei, Andrew Y.Ng, and Michae l. Jordan to find latent topics in a text corpus. \n",
    "\n",
    "  This algorithm is a three-level hierarchical Bayesian model. It has three basic concepts: word, topic, and document. Actually, LDA mimics the method of human write an article and uses the process to reverse inference the distribution of topics when we have an article.Variational Inference is used for approximating intractable integrals arising in a Bayesian network. Therefore, in the inference steps, variational Inference can be seen as an extension of the EM algorithm which computes the entire posterior distribution of latent variables. In the meanwhile, LDA uses Newton-Raphson method and gradient descent to calculate the minimum of a lower bound. Also, the Hessian matrix is necessary for gradient descent procedures.\n",
    "\n",
    "\n",
    "  In this report, we described, analyzed and mathematically implemented the LDA model. And then implement all the core functions in LDA algorithm. Additionally, we used Spark to optimize the functions. Finally, a simulation data was used to test the accuracy of our functions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background\n",
    "\n",
    "## 2.1 Bayesian Model\n",
    "prior + likelihood = posterior\n",
    "\n",
    "\n",
    "## 2.2 Dirichlet distribution and Multinomial distribution\n",
    "\n",
    "Conjugate distribution intro....\n",
    "$$\n",
    "multi(m_1,m_2,m_3\\mid n, p_1,p_2,p_3) = \\frac{n!}{m_1!m_2!m_3!}p_1^{m_1}p_2^{m_2}p_3^{m_3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Dirichlet(p_1,p_2,p_3\\mid \\alpha_1,\\alpha_2,\\alpha_3) = \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}p_1^{\\alpha_1-1}p_2^{\\alpha_2-1}p_3^{\\alpha_3-1}\n",
    "$$\n",
    "\n",
    "For higher dimension, we have\n",
    "\n",
    "$$\n",
    "Dirichlet(\\vec{p}\\mid \\vec{\\alpha}) = \\frac{\\Gamma(\\sum_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\\prod_{k=1}^K\n",
    "p_k^{\\alpha_k-1}$$\n",
    "\n",
    "With the same conjugate relationship, we have:\n",
    "\n",
    "$$\n",
    "Dirichlet(\\vec{p}\\mid\\vec{\\alpha})+Multi(\\vec{m}) = Dirichlet(\\vec{p}\\mid\\vec{\\alpha}+\\vec{m})\n",
    "$$\n",
    "\n",
    "And the expectation of Dirichlet distribution is:\n",
    "\n",
    "$$\n",
    "E(Diri(\\vec{p}\\mid\\vec{\\alpha}) = \\left( \\frac{\\alpha_1}{\\sum_{k=1}^K\\alpha_k},\\frac{\\alpha_2}{\\sum_{k=1}^K\\alpha_k},...\\frac{\\alpha_K}{\\sum_{k=1}^K\\alpha_k}  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 LDA Model\n",
    "\n",
    "  We use the language of text collections throughout the paper, referring to entities such as “words,” “documents,” and “corpora.”(p995) Our goal is to find the topic dictribution of each document and the word distribution of each topic. Also, if we are given a new document, we need to use some parameters trained from our model, to inference the topic of taht new document.\n",
    "  \n",
    "  To mimic the process of human wrinting an article, the author always needs to have some fixed topics and then uses words withch are strongly related with those topics. So when it comes to machine learning, we also pretend that we need to have the topic at first and then we can generate words under that topic.\n",
    "\n",
    "To describe the algorithm in an aesier way to understanding, we can treat distribution as a multi-sided dice, each side represents a topic(word), the probabilities of being up of each side are defferent. Added Bayesian idea, the LDA algorihm can be showed as:\n",
    "- There are two big boxes, one is full of \"doc-topic\" dices, and another box is full of \"topic-word\" dices.\n",
    "- Pick $K$ \"topic-word\" dices from the second box, label tham as $1,2,...,K$.\n",
    "- For each document, we pick one \"doc-topic\" dice from the first box and repeat followings to get words:\n",
    "   - through this dice, obtain the topic $z$\n",
    "   - chooes the \"doc-topic\" dice whose lable is $z$ and through it, then we can obtain one word\n",
    "   \n",
    "   \n",
    "In other words, LDA algorithm assumes the following generative process for each document $\\overrightarrow{w}$:\n",
    "\n",
    "1. Choose $N \\sim Poisson(\\xi)$\n",
    "2. Choose $\\theta \\sim Dir(\\alpha)$\n",
    "3. For each of the $N$ words $w_n$:\n",
    "\n",
    "    (a). Choose a topic $z_n \\sim Multi(\\theta)$\n",
    "    \n",
    "    (b). Choose a word $w_n$ from $p(w_n\\mid z_n, \\beta)$, a multinomal probability conditioned on th etopic $z_n$.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Mathematical analysis\n",
    "\n",
    "First of all, let's define some terms which will use in the following text:\n",
    "\n",
    "$\\cdot$ $D$(a corpus) is the collection of $M$ documents;\n",
    "\n",
    "$\\cdot$ $V$ is the total number of words from a vocabulary;\n",
    "\n",
    "$\\cdot$ $M$ is the number of documents;\n",
    "\n",
    "$\\cdot$ $K$ is number of topics of the corpus;\n",
    "\n",
    "$\\cdot$ $N_m$ is the number of words in the $m^{th} document$;\n",
    "\n",
    "$\\cdot$ $w_{mn}$ is the  $n^{th}$ word in docuent $m$; \n",
    "\n",
    "$\\cdot$ $z_{mn}$ is the topic of word $w_{mn}$;\n",
    "\n",
    "$\\cdot$ $\\theta_m$ is the distribution for document $m$;\n",
    "\n",
    "$\\cdot$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that the prior distribution of topics is a Dirichlet distribution, i.e., for any document $d$, the distribution of its topic is\n",
    "\n",
    "$$\n",
    "\\theta_d = Dirichlet(\\vec{\\alpha})\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a K-dimension vector.\n",
    "\n",
    "A k-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector  lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i=1$), and has the following probability density on the simplex:\n",
    "$$\n",
    "p(\\theta\\mid \\alpha) = \\frac{\\Gamma(\\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\theta_1^{\\alpha_1-1}...\\theta_k^{\\alpha_k-1}\n",
    "$$\n",
    "\n",
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\textbf{z} = (z_1, z_2,...z_N)$ and a set of $N$ words $\\textbf{w} = (w_1, w_2,...w_N)$ is given by:\n",
    "$$\n",
    "p(\\theta, \\textbf{z},\\textbf{w}\\mid \\alpha, \\beta) = p(\\theta\\mid\\alpha)\\prod_{n=1}^Np(z_n\\mid \\theta)p(w_n\\mid z_n,\\beta)\n",
    "$$\n",
    "\n",
    "Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document $\\textbf{w}$:\n",
    "$$\n",
    "p(\\textbf{w}\\mid \\alpha, \\beta) = \\int p(\\theta\\mid\\alpha)\\prod_{n=1}^N \\sum_{z_n} p(z_n\\mid \\theta)p(w_n\\mid z_n,\\beta)d\\,\\theta\n",
    "$$\n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus $\\textbf{D} = (\\textbf{w}_1, \\textbf{w}_2,...\\textbf{w}_M)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\textbf{D}\\mid \\alpha,\\beta) &= \\prod_{d=1}^M p(\\textbf{w}_d\\mid \\alpha,\\beta)\\\\\n",
    "&= \\prod_{d=1}^M \\int p(\\theta_d\\mid\\alpha) \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}\\mid \\theta)p(w_{dn}\\mid z_{dn},\\beta)d\\,\\theta_d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Variational inference (E-step)\n",
    "\n",
    "### Parameter declaration\n",
    "\n",
    "v: vocabulary, $v = 1,2,...,V$\n",
    "\n",
    "z: topic , $z = 1,2,...,K$\n",
    "\n",
    "m: document, $m = 1,2,...,M$\n",
    "\n",
    "w: word, $w =1,2,...N_1, N_1+1, ...N_1+N_2,...,\\sum_{m=1}^NN_m$ \n",
    "\n",
    "$\\alpha$: a $K$ dim vector, represents the topic distribution of $K$ topics\n",
    "\n",
    "$\\beta$: a $K\\times V$ matrix, $\\beta_{ij}$ represents probability of the $j^{th}$ word of $i^{th}$ topic.\n",
    "\n",
    "$\\phi$: a $M\\times N \\times K$ matrix, where $N = \\sum_{m=1}^NN_m$, represents probability of each topic for words in each document\n",
    "\n",
    "$\\gamma$: a $M\\times K$ matrix, represents the probability of topic for each document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We need to compute the posterior distribution of hidden variables:\n",
    "\n",
    "$$\n",
    "p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta) = \\frac{p(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)}{p(\\textbf{w}\\mid \\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "But this distribution is intractable due to the coupling between $\\theta$ and $\\beta$.\n",
    "So we use the variation distribution on latent variables:\n",
    "\n",
    "$$\n",
    "q(\\theta, \\textbf{z}\\mid \\gamma, \\phi) = q(\\theta\\mid \\gamma)\\prod_{n=1}^N q(z_n\\mid \\phi_n)\n",
    "$$\n",
    "\n",
    "An optimization problem that determins the values of $\\gamma$ and $\\phi$ with respect to KL-Divergence:\n",
    "\n",
    "$$\n",
    "(\\gamma^*, \\phi^*) = arg\\ \\underset{\\phi, \\gamma}{min}\\,D(q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta))\n",
    "$$\n",
    "\n",
    "Then we denote $q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)$ by $q$, the KL-Divergence between $q$ and $p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D(q||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)) &= E_q[log\\,q]-E_q[log\\,p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta)]\\\\\n",
    "&=E_q[log\\,q]-E_q[log\\,p(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)]+log\\,p(\\textbf{w}\\mid \\alpha, \\beta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using Jensen's inequality, we have the bound as $log\\,p(\\textbf{w}\\mid \\alpha, \\beta)\\geq E_q[log\\,p(\\theta, \\textbf{z},\\textbf{w}\\mid \\alpha, \\beta)]-E_q[log(\\theta, \\textbf{z})]$. And by denoting $E_q[log(\\theta, \\textbf{z}, \\textbf{w}\\mid \\alpha, \\beta)] - E_q[log\\,q(\\theta, \\textbf{z})]$ by $L(\\gamma, \\phi; \\alpha, \\beta)$, we have\n",
    "\n",
    "$$\n",
    "log\\,P(\\textbf{w}\\mid \\alpha, \\beta) = L(\\gamma, \\phi; \\alpha, \\beta)+D(q(\\theta, \\textbf{z}\\mid \\gamma, \\phi)||p(\\theta, \\textbf{z}\\mid \\textbf{w}, \\alpha, \\beta))\n",
    "$$\n",
    "\n",
    "So minimizeing the KL-Divergence $D$ is equivalent to maximizing likelihood $L$, then we need to maximize the following five parts one by one:\n",
    "\n",
    "$$\n",
    "L(\\gamma, \\phi;\\alpha, \\beta) = E_q[log\\,p(\\theta\\mid \\alpha)]+E_q[log\\,p(\\textbf{z}\\mid\\theta)]+E_q[log\\,p(\\textbf{w}\\mid\\textbf{z},\\beta)]-E_q[log\\,q(\\theta)-E_q[log\\,q(\\textbf{z})]]\n",
    "$$\n",
    "\n",
    "Finally, by computing the derivatives of $L$ and set them equals to 0, we obtain update equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi_{ni}&\\propto \\beta_{iv}\\,exp\\{\\psi(\\gamma_i)-\\psi(\\sum_{j=1}^K\\gamma_j)\\}\\\\\n",
    "\\gamma_i &= \\alpha_i+\\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\psi$ is the digamma function, the first derivative of the log Gamma function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference Algorithm\n",
    "\n",
    "- initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all $i$ and $n$.\n",
    "- initialize $\\gamma_i^0 = \\alpha_i+\\frac{N}{K}$ for all $i$.\n",
    "- repeat\n",
    "    - for $n = 1$ to $N$\n",
    "    - for $i = 1$ to $K$\n",
    "         - $\\phi_{ni}^{t+1} = \\beta_{iw_n}\\,exp\\{\\psi(\\gamma_i^t)\\}$.\n",
    "         - normalize $\\phi_{ni}^{t+1}$ to sum 1.\n",
    "    - $\\gamma^{t+1} = \\alpha+\\sum_{n=1}^N \\phi_n^{t+1}$.\n",
    "- until convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence function\n",
    "def is_convergence1(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(list(map(np.sum,np.square(old - new))))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "def is_convergence2(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(np.sum(np.square(old - new)))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "def optimize_vp(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        gamma_old = gamma\n",
    "        #update phi\n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                for i in range(K):\n",
    "                    phi[m][n,i] = beta[i,np.int(words[m][n])] * np.exp(digamma(gamma[m,i]))\n",
    "                #nomalize to 1\n",
    "                phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "        phi_new = phi\n",
    "        #update gamma\n",
    "        for i in range(M):\n",
    "            gamma[i,:]  = alpha + np.sum(phi[i], axis = 0)\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1(phi_old, phi_new) == True and is_convergence2(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.3 Parameter estimation (M-step)\n",
    "\n",
    "Now, given a corpus of documents $\\textbf{D} = (\\textbf{w}_1, \\textbf{w}_2,...\\textbf{w}_M)$, we hope to find proper parameters. So, in this M-step, maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$.\n",
    "\n",
    "Firstly, we maximize $L(\\gamma, \\phi;\\alpha,\\beta)$ with respect to $\\beta$, by taking the derivative with respect to $\\beta_{ij}$ and setting it to zero, we have:\n",
    "\n",
    "$$\n",
    "\\beta_{ij}\\propto \\sum_{m=1}^M\\sum_{n=1}^{N_m}\\phi_{m,n,i}w_{mn}^j\n",
    "$$\n",
    "\n",
    "here we need to attention that $\\beta$ is a probability matrix whose rows represent words probability of one particular topic, so the sum of each rows should equal one.\n",
    "\n",
    "Secondly, we maximize $L(\\gamma, \\phi;\\alpha,\\beta)$ with respect to $\\alpha$, and then take the derivative with respect to $\\alpha_i$. However, it is difficult to compute $\\alpha_i$ by setting the derivative to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} = M\\left(\\psi(\\sum_{j=1}^K \\alpha_i)-\\psi(\\alpha_i)\\right)+\\sum_{m=1}^M \\left(\\psi_{mi}-\\psi\\left(\\sum_{j=1}^K \\gamma_{mj}\\right)\\right):=g_i\n",
    "$$\n",
    "\n",
    "Because this derivative depends on $\\alpha_i$, where $j\\neq i$. So we need to use an iterative method (eg. Newton-Raphson method) to find the extreme point. Then we can compute the Hessian Matrix as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2L}{\\partial \\alpha_i\\partial\\alpha_j} = -\\delta(i,j)M\\psi'(\\alpha_i) +M \\psi'\\left(\\sum_{j=1}^K \\alpha_i\\right):=H_{ij}\n",
    "$$\n",
    "\n",
    "Finally, we only need to use Newton-Raphson methods to update $\\alpha$ and obtain the answer when converge. The Newton-Raphson optimization technique finds a stationary point of a function by iterating\n",
    "\n",
    "$$\n",
    "\\alpha_{new} = \\alpha_{old}-H(\\alpha_{old})^{-1}g(\\alpha_{old})\n",
    "$$\n",
    "\n",
    "where $H(\\alpha)$ is the Hessian matrix and $g(\\alpha)$ is the gradient vector.\n",
    "If the Hessian matrix is of form $H = diag(h)+\\textbf{1}\\,z\\,\\textbf{1}^T$, then we can culculate that\n",
    "\n",
    "$$\n",
    "(H^{-1}g)_i = \\frac{g_i-c}{h_i}\\\\\n",
    "c = \\frac{\\sum_{j=1}^K g_j/h_j}{z^{-1}+\\sum_{j=1}^K h_j^{-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alpha\n",
    "def alpha_estimate(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    from scipy.special import digamma, polygamma\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # compute the gradient vector and the diagonal part of the Hessian matrix\n",
    "        g = np.zeros(K)\n",
    "        h = np.zeros(K)\n",
    "        for i in range(K):\n",
    "            g1 = M*(digamma(np.sum(alpha))-digamma(alpha[i]))\n",
    "            g2 = 0\n",
    "            for d in range(M):\n",
    "                g2 += digamma(gamma[d,i])-digamma(np.sum(gamma[d,:]))\n",
    "            g[i] = g1 + g2\n",
    "            \n",
    "            h[i] = -M*polygamma(1, alpha[i])\n",
    "        \n",
    "        # compute the constant part\n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate beta\n",
    "def beta_estimate(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one\n",
    "    for i in range(K):\n",
    "        beta[i,:] = beta[i,:]/sum(beta[i,:])\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Combination (Variational EM)\n",
    "\n",
    "After the procedures above, now we can combine all the algorithms into a summary:\n",
    "\n",
    "1. (E-step) For each document, find the optimizing values of the variational parameters ${\\gamma_d^*, \\phi)d^*:\\,d\\in D}$. This is done as described in the previous section.\n",
    "\n",
    "2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "\n",
    "These two steps are repeated until the lower bound on the log likelihood converges.\n",
    "\n",
    "\n",
    "### (1) input:\n",
    "- K: the number of topics\n",
    "- M: the number of documents\n",
    "- D: contains all words of documents\n",
    "\n",
    "### (2) initialize $\\alpha$ and $\\beta$\n",
    "\n",
    "### (3) start EM algorithm:\n",
    "- Use variational inference algorithm to update $\\gamma$ and $\\phi$:\n",
    "- (1). initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all $i$ and $n$.\n",
    "- (2). initialize $\\gamma_i^0 = \\alpha_i+\\frac{N}{K}$ for all $i$.\n",
    "\n",
    "   - repeat\n",
    "   - for $n = 1$ to $N$\n",
    "   - for $i = 1$ to $K$\n",
    "       - $\\phi_{ni}^{t+1} = \\beta_{iw_n}\\,exp\\{\\psi(\\gamma_i^t)\\}$.\n",
    "       - normalize $\\phi_{ni}^{t+1}$ to sum 1.\n",
    "   - $\\gamma^{t+1} = \\alpha+\\sum_{n=1}^N \\phi_n^{t+1}$.\n",
    "   - until convergence\n",
    "     \n",
    "- Use parameter estimation to estimate $\\alpha$ and $\\beta$:\n",
    "   - use the converged $\\phi$ and $\\gamma$ to update\n",
    "   - until convergence\n",
    "   \n",
    "### (4) when all parameters ocnvergede, break.\n",
    "\n",
    "### (5) output posterior parameters $\\alpha$ and $\\beta$, end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation EM\n",
    "def variation_EM(M, K, D, N, V_words, alpha_initial, beta_initial, gamma_initial, phi_initial, iteration = 1000):\n",
    "    \n",
    "    phi_gamma = optimize_vp(phi_initial, gamma_initial, alpha_initial, beta_initial, w_struct, M, N, K)\n",
    "    phi = phi_gamma[0]\n",
    "    gamma = phi_gamma[1]\n",
    "    \n",
    "     \n",
    "    (alpha, beta) = (alpha_initial, beta_initial)\n",
    "    \n",
    "    for t in range(iteration):\n",
    "        \n",
    "        (phi_old, gamma_old) = (phi, gamma)\n",
    "        \n",
    "        alpha = alpha_estimate(gamma, alpha, K, M)\n",
    "        beta = beta_estimate(K, V_words, phi, D)\n",
    "        \n",
    "        phi_gamma1 = optimize_vp(phi, gamma, alpha, beta, w_struct, M, N, K)\n",
    "        phi = phi_gamma1[0]\n",
    "        gamma = phi_gamma1[1]\n",
    "        \n",
    "        if is_convergence2(gamma_old, gamma) and is_convergence1(phi_old, phi):\n",
    "            break\n",
    "    \n",
    "    return alpha, beta, gamma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Algorithm optimization\n",
    "Now we hope to improve our functions in order to speed up. When look back at the algorithm, we find there are some places that can be replaced by other optimal methods.\n",
    "\n",
    "\n",
    "### Normalize $\\phi$\n",
    "Firts of all ,we notice that in the process of updating $\\phi$'s we need to normalize them. There is a famous tip which focuses on the transformation between \"log\" and \"sum\". In other words, we can use $log(a)$ and $log(b)$ to calculate $log(a+b)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "log(a+b) &= log\\left(a\\times (1+\\frac{b}{a})\\right)\\\\\n",
    "&=log(a)+log\\left(a+\\frac{b}{a}\\right)\\\\\n",
    "&= log(a)+log\\left(1+e^{log(b)-log(a)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So in normalization, this method can add in pairs during each iteration after geting $log(\\phi_1), log(\\phi_2),...,log(\\phi_K)$ and finally speed up the summation of $log(\\phi_1+\\phi_2+...+\\phi_K)$. And then use the following equation to realize normalization:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\frac{\\phi_i}{\\phi1+\\phi_2+...+\\phi_K}\\\\\n",
    "\\Rightarrow log(\\phi_i) = \\log(\\phi_i)-\\log(\\phi_1+\\phi_2+...+\\phi_K)\n",
    "$$\n",
    "\n",
    "Alos, because of the target value has become $log(\\phi_1)$, the iterative formula of $\\phi$ should be changed into:\n",
    "\n",
    "$$\n",
    "log(\\phi_{dni}^{t+1}) = log(\\beta_{i,w_n})+\\psi(\\gamma_{di}^t)\n",
    "$$\n",
    "\n",
    "### Update $\\gamma$\n",
    "\n",
    "After obtaining $\\phi_{ni}$, we find another method to obtain $\\gamma_i$ in an easier way.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_i^0 &= \\alpha_i+\\frac{N}{K}\\\\\n",
    "\\gamma_i^{t+1} &= \\alpha_i+\\sum_{n=1}^N \\phi_{dn}^{t+1}\\\\\n",
    "&=\\alpha_i+\\sum_{n=1}^N (\\phi_n^{t}+\\bigtriangleup \\phi_{dn}^{t+1})\\\\\n",
    "&=(\\alpha_i+\\sum_{n=1}^N \\phi_n^{t})+\\sum_{n=1}^N\\bigtriangleup \\phi_{dn}^{t+1})\\\\\n",
    "&=\\gamma_i^t+\\sum_{n=1}^N\\left(\\phi_{dn}^{t+1}-\\phi_{dn}^{t}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, we can use only $\\gamma^t$, $\\phi^t$ and $\\phi^{t+1}$ to calculate a new $\\gamma^{t+1}$, without using $\\alpha$. This can be explained by the truth that we have taken account of $\\alpha$ in the initialization step, the information of $\\alpha$ has been included in $\\gamma$. \n",
    "\n",
    "### Vectorizing loops\n",
    "\n",
    "In our original codes, there are many for loops which may cause a slow computing. However, vectors are a great helper. We will implement this method in functions of eatimating $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new function to calculate log of sum\n",
    "def log_sum(log_a, log_b):\n",
    "    \"\"\"\n",
    "    input: log(a), log(b)\n",
    "    output: log(a+b)\n",
    "    \"\"\"\n",
    "    return log_a + np.log(1+np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "\n",
    "def optimize_vp_opt(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        \n",
    "        # we use log(phi) here and following processes\n",
    "        log_phi = np.array(list(map(np.log, phi)))\n",
    "        gamma_old = gamma\n",
    "       \n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                \n",
    "                logsum = 0\n",
    "                for i in range(K):\n",
    "                    \n",
    "                    # use new method in log form to update phi\n",
    "                    log_phi[m][n,i] = np.log(beta[i,np.int(words[m][n])]) + digamma(gamma[m,i])\n",
    "                    \n",
    "                    logsum = log_sum(logsum, log_phi[m][n,i])\n",
    "                # use new metohd to implement nomalization\n",
    "                log_phi_mn = log_phi[m][n,:] - logsum\n",
    "                log_phi[m][n,:] = log_phi_mn\n",
    "                \n",
    "                phi[m][n,:] = np.exp(log_phi_mn)\n",
    "        \n",
    "            # instead of alpha, use old phi and new phi to iterative\n",
    "            d_phi = phi[m] - phi_old[m]\n",
    "            gamma[m,:]  = gamma[m,:] + np.sum(d_phi, axis = 0)\n",
    "            \n",
    "        phi_new = phi\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1(phi_old, phi_new) == True and is_convergence2(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimate alpha\n",
    "def alpha_estimate_opt(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    from scipy.special import digamma, polygamma\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # we use vector instead of calculating in loop\n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) \n",
    "        + np.sum(digamma(gamma) -np.tile(digamma(np.sum(gamma,axis=1)),(K,1)).T,axis=0)\n",
    "        h = -M*polygamma(1,alpha)\n",
    "        \n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate beta\n",
    "def beta_estimate_opt(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one, in vector method\n",
    "    beta= beta/ np.sum(beta, axis = 1).reshape((-1,1))\n",
    "        \n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 JIT optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dv = rc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numba on engine(s)\n",
      "importing numpy on engine(s)\n",
      "importing random on engine(s)\n"
     ]
    }
   ],
   "source": [
    "with dv.sync_imports():\n",
    "    import numba\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "@numba.jit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Experimental test\n",
    "\n",
    "### Simulating Data\n",
    "\n",
    "Now we'd like to check whether our implementation is correct and whteher the whole functions can give an appropriate reslut. So we generate a set of data which contains 520 documents. And in each document, the number of words is in the range from 100 to 300. We assume the number of topics is 6. In order to visualize the distribution much clearer, we would devide the 520 documents into 6 parts, each parts has the same preference of pair topics. \n",
    "\n",
    "$$\n",
    "\\textbf{D} = \\{ (\\textbf{w}_1,...\\textbf{w}_{90}), (\\textbf{w}_{91},...\\textbf{w}_{180}), (\\textbf{w}_{181},...\\textbf{w}_{270}), (\\textbf{w}_{271},...\\textbf{w}_{360}), (\\textbf{w}_{361},...\\textbf{w}_{450}), (\\textbf{w}_{451},...\\textbf{w}_{520}) \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Topics = \\{(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Vocabulary = \\{a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,\\\\\n",
    "A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(180422)\n",
    "\n",
    "M = 520\n",
    "K = 6\n",
    "V_word =  np.array(list([chr(i) for i in range(97,123)]) + list([chr(i) for i in range(65,91)]))\n",
    "N = np.random.randint(100,300,size=M)\n",
    "V = len(V_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(180)\n",
    "\n",
    "# generate probability distribution for six types of topic preferences\n",
    "alpha = np.fromfunction(lambda i,j: np.where((i-j==-1) | (i==j), np.random.randint(15, 30, 6), 1), (6,6), dtype = 'int')\n",
    "alpha[5,0] = np.random.randint(15, 30)\n",
    "beta_preference = (np.ones((K, V)) + np.array([np.arange(V)%K==i for i in range(K)])*10)/16\n",
    "beta_true = np.array(list(map(lambda x: np.random.dirichlet(x),beta_preference.T))).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(beta_preference, interpolation='nearest', aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(beta_true, interpolation='nearest', aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "M = 300\n",
    "k = 10\n",
    "N = np.random.randint(150,200,size=M)\n",
    "V = 30\n",
    "\n",
    "# Create 3 groups of documents, each with a topic preference\n",
    "alpha_gen1 = np.array((20,15,10,1,1,1,1,1,1,1))\n",
    "alpha_gen2 = np.array((1,1,1,10,15,20,1,1,1,1))\n",
    "alpha_gen3 = np.array((1,1,1,1,1,1,10,12,15,18))\n",
    "\n",
    "# Arbitrarily choose each topic to have 3 very common words\n",
    "beta_probs = np.ones((V,k)) + np.array([np.arange(V)%k==i for i in range(k)]).T*19\n",
    "beta_gen = np.array(list(map(lambda x: np.random.dirichlet(x),beta_probs.T))).T\n",
    "\n",
    "w_struct = list();\n",
    "theta = np.empty((M,k))\n",
    "\n",
    "# Generate each document\n",
    "for m in range(M):\n",
    "    # Draw topic distribution for the document\n",
    "    if m<M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen1,1)[0]\n",
    "    elif m<2*M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen2,1)[0]\n",
    "    else:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen3,1)[0]\n",
    "    doc = np.array([])\n",
    "    \n",
    "    for n in range(N[m]):\n",
    "        # Draw topic according to document's topic distribution\n",
    "        z_n = np.random.choice(np.arange(k),p=theta[m,:])\n",
    "        # Draw word according to topic\n",
    "        w_n = np.random.choice(np.arange(V),p=beta_gen[:,z_n])\n",
    "        doc = np.append(doc,w_n)\n",
    "    w_struct.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
