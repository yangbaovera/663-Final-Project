{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jovyan/work/latent-dirichlet-allocation\n",
      "Building wheels for collected packages: lda-package\n",
      "  Running setup.py bdist_wheel for lda-package ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/55/df/03/5c2615a45fff840cc1dac92c4e9d5fa0e8c00f0d88616624ee\n",
      "Successfully built lda-package\n",
      "Installing collected packages: lda-package\n",
      "  Found existing installation: lda-package 1.0\n",
      "    Uninstalling lda-package-1.0:\n",
      "      Successfully uninstalled lda-package-1.0\n",
      "Successfully installed lda-package-1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation- Optimization Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize our model, we consider two different aspects- Algorithm Optimization and Code Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithm Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Normalization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating $\\phi$, we need to normalize $\\phi$ to make it a probabiliy.\n",
    "So we have,$$\\phi_i = \\frac{\\phi_i}{\\phi1+\\phi_2+...+\\phi_K}$$\n",
    "Actually we can use $\\log$ function to optimize this process,\n",
    "$$log(\\phi_i) = \\log(\\phi_i)-\\log(\\phi_1+\\phi_2+...+\\phi_K$$\n",
    "\n",
    "So in normalization, this method can add in pairs during each iteration after geting $log(\\phi_1), log(\\phi_2),...,log(\\phi_K)$ and finally speed up the summation of $log(\\phi_1+\\phi_2+...+\\phi_K)$\n",
    "And because of the target value has become $log(\\phi_1)$, the iterative formula of $\\phi$ should be changed into:\n",
    "\n",
    "$$\n",
    "log(\\phi_{dni}^{t+1}) = log(\\beta_{i,w_n})+\\psi(\\gamma_{di}^t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(K):   \n",
    "#     log_phi[m][n,i] = np.log(beta[i,np.int(words[m][n])]) + digamma(gamma[m,i])                \n",
    "#     logsum = log_sum(logsum, log_phi[m][n,i])\n",
    "# log_phi_mn = log_phi[m][n,:] - logsum              # use new metohd to implement nomalization\n",
    "# log_phi[m][n,:] = log_phi_mn\n",
    "                \n",
    "# phi[m][n,:] = np.exp(log_phi_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize $\\gamma$ Updating Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are updating the $\\gamma$, we use\n",
    "$$\\gamma_i^0 = \\alpha_i+\\frac{N}{K}\\\\\n",
    "\\gamma_i^{t+1} = \\alpha_i+\\sum_{n=1}^N \\phi_{dn}^{t+1}$$\n",
    "But we can find that the $\\gamma^t$ has the $\\alpha$ information $\\gamma^{t+1}$ needs, so we can optimize this process\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\gamma_i^{t+1} &= \\alpha_i+\\sum_{n=1}^N \\phi_{dn}^{t+1}\\\\\n",
    "&=\\alpha_i+\\sum_{n=1}^N (\\phi_n^{t}+\\bigtriangleup \\phi_{dn}^{t+1})\\\\\n",
    "&=(\\alpha_i+\\sum_{n=1}^N \\phi_n^{t})+\\sum_{n=1}^N\\bigtriangleup \\phi_{dn}^{t+1})\\\\\n",
    "&=\\gamma_i^t+\\sum_{n=1}^N\\left(\\phi_{dn}^{t+1}-\\phi_{dn}^{t}\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_phi = phi[m] - phi_old[m]\n",
    "# gamma[m,:]  = gamma[m,:] + np.sum(d_phi, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use vector/matrix manipulation to replace the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "#\n",
    "# for-loop\n",
    "# for i in range(K):\n",
    "#             g1 = M*(digamma(np.sum(alpha))-digamma(alpha[i]))\n",
    "#             g2 = 0\n",
    "#             for d in range(M):\n",
    "#                 g2 += digamma(gamma[d,i])-digamma(np.sum(gamma[d,:]))\n",
    "#             g[i] = g1 + g2\n",
    "# Vector\n",
    "# g = M*(digamma(np.sum(alpha))-digamma(alpha)) \n",
    "#         + np.sum(digamma(gamma) -np.tile(digamma(np.sum(gamma,axis=1)),(K,1)).T,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Complete Code for Algorithm Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new function to calculate log of sum\n",
    "def log_sum(log_a, log_b):\n",
    "    \"\"\"\n",
    "    input: log(a), log(b)\n",
    "    output: log(a+b)\n",
    "    \"\"\"\n",
    "    return log_a + np.log(1+np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "\n",
    "def optimize_vp_opt(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        \n",
    "        # we use log(phi) here and following processes\n",
    "        log_phi = np.array(list(map(np.log, phi)))\n",
    "        gamma_old = gamma\n",
    "       \n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                \n",
    "                logsum = 0\n",
    "                for i in range(K):\n",
    "                    \n",
    "                    # use new method in log form to update phi\n",
    "                    log_phi[m][n,i] = np.log(beta[i,np.int(words[m][n])]) + digamma(gamma[m,i])\n",
    "                    \n",
    "                    logsum = log_sum(logsum, log_phi[m][n,i])\n",
    "                # use new metohd to implement nomalization\n",
    "                log_phi_mn = log_phi[m][n,:] - logsum\n",
    "                log_phi[m][n,:] = log_phi_mn\n",
    "                \n",
    "                phi[m][n,:] = np.exp(log_phi_mn)\n",
    "        \n",
    "            # instead of alpha, use old phi and new phi to iterative\n",
    "            d_phi = phi[m] - phi_old[m]\n",
    "            gamma[m,:]  = gamma[m,:] + np.sum(d_phi, axis = 0)\n",
    "            \n",
    "        phi_new = phi\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1(phi_old, phi_new) == True and is_convergence2(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alpha\n",
    "def alpha_estimate_opt(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    from scipy.special import digamma, polygamma\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # we use vector instead of calculating in loop\n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) \n",
    "        + np.sum(digamma(gamma) -np.tile(digamma(np.sum(gamma,axis=1)),(K,1)).T,axis=0)\n",
    "        h = -M*polygamma(1,alpha)\n",
    "        \n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate beta\n",
    "def beta_estimate_opt(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one, in vector method\n",
    "    beta= beta/ np.sum(beta, axis = 1).reshape((-1,1))\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda_package     #our package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "texts, dictionary, corpus = lda_package.data_clean(doc_set)\n",
    "text_ = lda_package.data_process(texts, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameter\n",
    "M = len(texts)\n",
    "k = 2\n",
    "N = np.array(list(map(len, text_)))\n",
    "V = len(dictionary)\n",
    "V_words = range(V)\n",
    "alpha = np.random.dirichlet(10*np.ones(k),1)[0]\n",
    "beta = np.random.dirichlet(np.ones(V),k)\n",
    "\n",
    "phi = np.array([1/k*np.ones([N[m],k]) for m in range(M)])\n",
    "gamma = np.tile(alpha,(M,1)) + np.tile(N/k,(k,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 14.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans1 = lda_package.variation_EM(M, k, text_, N, V_words, alpha, beta, gamma, phi, iteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 9.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans2 = lda_package.variation_EM_new(M, k, text_, N, V_words, alpha, beta, gamma, phi, iteration = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "\n",
    "def log_sum_jit(log_a, log_b):\n",
    "    \"\"\"\n",
    "    input: log(a), log(b)\n",
    "    output: log(a+b)\n",
    "    \"\"\"\n",
    "    return log_a + np.log(1+np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "\n",
    "def optimize_vp_opt_jit(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        \n",
    "        # we use log(phi) here and following processes\n",
    "        log_phi = np.array(list(map(np.log, phi)))\n",
    "        gamma_old = gamma\n",
    "       \n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                \n",
    "                logsum = 0\n",
    "                for i in range(K):\n",
    "                    \n",
    "                    # use new method in log form to update phi\n",
    "                    log_phi[m][n,i] = np.log(beta[i,np.int(words[m][n])]) + digamma(gamma[m,i])\n",
    "                    \n",
    "                    logsum = log_sum_jit(logsum, log_phi[m][n,i])\n",
    "                # use new metohd to implement nomalization\n",
    "                log_phi_mn = log_phi[m][n,:] - logsum\n",
    "                log_phi[m][n,:] = log_phi_mn\n",
    "                \n",
    "                phi[m][n,:] = np.exp(log_phi_mn)\n",
    "        \n",
    "            # instead of alpha, use old phi and new phi to iterative\n",
    "            d_phi = phi[m] - phi_old[m]\n",
    "            gamma[m,:]  = gamma[m,:] + np.sum(d_phi, axis = 0)\n",
    "            \n",
    "        phi_new = phi\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1_jit(phi_old, phi_new) == True and is_convergence2_jit(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "\n",
    "def is_convergence1_jit(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(list(map(np.sum,np.square(old - new))))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "def is_convergence2_jit(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(np.sum(np.square(old - new)))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "# estimate alpha\n",
    "def alpha_estimate_opt_jit(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    from scipy.special import digamma, polygamma\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # we use vector instead of calculating in loop\n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) \n",
    "        + np.sum(digamma(gamma) -np.tile(digamma(np.sum(gamma,axis=1)),(K,1)).T,axis=0)\n",
    "        h = -M*polygamma(1,alpha)\n",
    "        \n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2_jit(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "# estimate beta\n",
    "def beta_estimate_opt_jit(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one, in vector method\n",
    "    beta= beta/ np.sum(beta, axis = 1).reshape((-1,1))\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "\n",
    "def variation_EM_new_jit(M, K, D, N, V_words, alpha_initial, beta_initial, gamma_initial, phi_initial, iteration = 1000):\n",
    "    \n",
    "    phi_gamma = optimize_vp_opt_jit(phi_initial, gamma_initial, alpha_initial, beta_initial, D, M, N, K)\n",
    "    phi = phi_gamma[0]\n",
    "    gamma = phi_gamma[1]\n",
    "    \n",
    "     \n",
    "    (alpha, beta) = (alpha_initial, beta_initial)\n",
    "    \n",
    "    for t in range(iteration):\n",
    "        \n",
    "        (phi_old, gamma_old) = (phi, gamma)\n",
    "        \n",
    "        alpha = alpha_estimate_opt_jit(gamma, alpha, K, M)\n",
    "        beta = beta_estimate_opt_jit(K, V_words, phi, D)\n",
    "        \n",
    "        phi_gamma1 = optimize_vp_opt_jit(phi, gamma, alpha, beta, D, M, N, K)\n",
    "        phi = phi_gamma1[0]\n",
    "        gamma = phi_gamma1[1]\n",
    "        \n",
    "        if is_convergence2_jit(gamma_old, gamma) and is_convergence1_jit(phi_old, phi):\n",
    "            break\n",
    "    \n",
    "    return alpha, beta, gamma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2 ms ± 830 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ans3 = lda_package.variation_EM_new(M, k, text_, N, V_words, alpha, beta, gamma, phi, iteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 ms ± 290 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ans4 = variation_EM_new_jit(M, k, text_, N, V_words, alpha, beta, gamma, phi, iteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
